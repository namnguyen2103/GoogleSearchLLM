{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T15:36:25.877606Z",
     "iopub.status.busy": "2024-12-24T15:36:25.877318Z",
     "iopub.status.idle": "2024-12-24T15:36:40.246987Z",
     "shell.execute_reply": "2024-12-24T15:36:40.245945Z",
     "shell.execute_reply.started": "2024-12-24T15:36:25.877566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers langchain-community httpx tqdm sentence-transformers pyvi underthesea faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-24T15:36:40.248304Z",
     "iopub.status.busy": "2024-12-24T15:36:40.248058Z",
     "iopub.status.idle": "2024-12-24T15:36:57.010540Z",
     "shell.execute_reply": "2024-12-24T15:36:57.009830Z",
     "shell.execute_reply.started": "2024-12-24T15:36:40.248283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import httpx\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import re\n",
    "from typing import List\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "from IPython.display import display, Markdown\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import faiss\n",
    "from copy import deepcopy\n",
    "from underthesea import sent_tokenize\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T15:36:57.011998Z",
     "iopub.status.busy": "2024-12-24T15:36:57.011407Z",
     "iopub.status.idle": "2024-12-24T15:36:57.599575Z",
     "shell.execute_reply": "2024-12-24T15:36:57.598678Z",
     "shell.execute_reply.started": "2024-12-24T15:36:57.011973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "SEARCH_KEY = UserSecretsClient().get_secret(\"SEARCH_KEY\")\n",
    "# genai.configure(api_key=UserSecretsClient().get_secret(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T15:36:57.600754Z",
     "iopub.status.busy": "2024-12-24T15:36:57.600496Z",
     "iopub.status.idle": "2024-12-24T15:36:57.605850Z",
     "shell.execute_reply": "2024-12-24T15:36:57.605137Z",
     "shell.execute_reply.started": "2024-12-24T15:36:57.600722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_documents(documents: List[Document]) -> List[Document]:\n",
    "    processed_docs = []\n",
    "    for doc in documents:\n",
    "        # Remove excessive whitespace and newlines\n",
    "        text = re.sub(r'\\s+', ' ', doc.page_content).strip()\n",
    "\n",
    "        # Remove any remaining script or style tags if they were not already removed\n",
    "        text = re.sub(r'<script.*?</script>', '', text, flags=re.DOTALL)\n",
    "        text = re.sub(r'<style.*?</style>', '', text, flags=re.DOTALL)\n",
    "\n",
    "        # Remove any leftover special characters and whitespace\n",
    "        text = re.sub(r'[^\\w\\s.,?!()/;-]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        # Preserve the metadata (source, title, etc.)\n",
    "        processed_doc = Document(page_content=text, metadata=doc.metadata)\n",
    "        processed_docs.append(processed_doc)\n",
    "\n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T15:36:57.607073Z",
     "iopub.status.busy": "2024-12-24T15:36:57.606761Z",
     "iopub.status.idle": "2024-12-24T15:36:57.624116Z",
     "shell.execute_reply": "2024-12-24T15:36:57.623396Z",
     "shell.execute_reply.started": "2024-12-24T15:36:57.607052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Main function to search, extract URLs, and load documents\n",
    "def search_google(query: str, topk: int = 3, lan: str = 'vi', **params):\n",
    "    # Perform Google search\n",
    "    base_url = 'https://www.googleapis.com/customsearch/v1'\n",
    "    params = {\n",
    "        'key': API_KEY,\n",
    "        'cx': SEARCH_KEY,\n",
    "        'q': query,\n",
    "        'hl': lan,\n",
    "        **params\n",
    "    }\n",
    "    response = httpx.get(base_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    items = response.json().get('items', [])\n",
    "    urls = [item['link'] for item in items]\n",
    "\n",
    "    # Validate the URLs:\n",
    "    unique_urls=set()\n",
    "    search_urls=[]\n",
    "    \n",
    "    for url in urls:\n",
    "        if url.split(\"/\")[2] not in unique_urls:\n",
    "            try:\n",
    "                response = requests.get(url, timeout=1) \n",
    "                status_code = response.status_code\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:              \n",
    "                status_code = None\n",
    "                \n",
    "            if status_code == 200:\n",
    "                unique_urls.add(url.split(\"/\")[2])\n",
    "                print(f\"Currently searching the website: {url}\")\n",
    "                search_urls.append(url) \n",
    "                \n",
    "            if len(unique_urls) >= topk:\n",
    "                break\n",
    "    \n",
    "    if search_urls:\n",
    "        # Load documents from the URLs\n",
    "        loader = WebBaseLoader(\n",
    "            web_paths=search_urls,  # Limit to top 3 URLs\n",
    "            bs_kwargs=dict(parse_only=bs4.SoupStrainer()),\n",
    "            requests_kwargs={\"timeout\": 600}\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        return preprocess_documents(docs)\n",
    "\n",
    "        # # Create a formatted string of document data\n",
    "        # doc_string = \"\\n\\n\".join(\n",
    "        #     f\"URL {index + 1}\\n\"\n",
    "        #     f\"Source: {doc.metadata.get('source', 'N/A')}\\n\"\n",
    "        #     f\"Title: {doc.metadata.get('title', 'N/A')}\\n\"\n",
    "        #     f\"Description: {doc.metadata.get('description', 'N/A')}\\n\"\n",
    "        #     f\"Content: {doc.page_content}\\n\"\n",
    "        #     for index, doc in enumerate(docs)\n",
    "        # )\n",
    "\n",
    "        # return doc_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T15:36:57.626461Z",
     "iopub.status.busy": "2024-12-24T15:36:57.626254Z",
     "iopub.status.idle": "2024-12-24T15:37:02.606774Z",
     "shell.execute_reply": "2024-12-24T15:37:02.605860Z",
     "shell.execute_reply.started": "2024-12-24T15:36:57.626443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently searching the website: https://vi.wikipedia.org/wiki/Pickleball\n",
      "Currently searching the website: https://nhathuoclongchau.com.vn/bai-viet/pickleball-la-gi-luat-choi-va-ky-thuat-co-ban-ban-can-nam.html\n",
      "Currently searching the website: https://irace.vn/pickleball-la-gi/\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=2000, \n",
    "    chunk_overlap=0,\n",
    "    separator=\" \",\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "docs = search_google('pickleball là gì', 3, 'vi')\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T15:37:02.608465Z",
     "iopub.status.busy": "2024-12-24T15:37:02.608142Z",
     "iopub.status.idle": "2024-12-24T15:37:02.613088Z",
     "shell.execute_reply": "2024-12-24T15:37:02.612140Z",
     "shell.execute_reply.started": "2024-12-24T15:37:02.608435Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def splits_to_dicts(splitted_docs):\n",
    "    doc_dicts = []\n",
    "    for idx, doc in enumerate(splitted_docs):\n",
    "        doc_dict = {\n",
    "            \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "            \"title\": doc.metadata.get(\"title\", \"No Title\"),\n",
    "            \"passage\": doc.page_content,\n",
    "            \"id\": idx\n",
    "        }\n",
    "        doc_dicts.append(doc_dict)\n",
    "    return doc_dicts\n",
    "\n",
    "corpus = splits_to_dicts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Vietnamese embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T15:37:02.614078Z",
     "iopub.status.busy": "2024-12-24T15:37:02.613837Z",
     "iopub.status.idle": "2024-12-24T15:37:15.907208Z",
     "shell.execute_reply": "2024-12-24T15:37:15.906469Z",
     "shell.execute_reply.started": "2024-12-24T15:37:02.614058Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e9ccdbe368422790d75ebb9a701c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46875ba0eb074b4d80a1a7bf2d0746d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f885a67edf43c99bb43815d54ab87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287bc10533e64d9a8ec94b124d9eddb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be297c219da84deba6f1fa0b63dd19f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f688b01192b24669b425da16f22c4d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973c209584ff47d5bf8ea70402c2bb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ca73eb7c0a45c08a6656a376de227a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4084587606ac48ebb43279d058453fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4148d4dae81f43b0bb39bab5a02629ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/22.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24225bfe5008435882c19f5302818f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418e70af39e8433eb2cabdbde3e94b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb_model = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T15:37:15.908317Z",
     "iopub.status.busy": "2024-12-24T15:37:15.908024Z",
     "iopub.status.idle": "2024-12-24T15:37:15.914303Z",
     "shell.execute_reply": "2024-12-24T15:37:15.913675Z",
     "shell.execute_reply.started": "2024-12-24T15:37:15.908292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def retrieve(query, corpus=corpus,topk=10):\n",
    "    segmented_corpus = [tokenize(example['passage']) for example in tqdm(corpus)]\n",
    "    embeddings = emb_model.encode(segmented_corpus, convert_to_numpy=True)\n",
    "    embeddings /= np.linalg.norm(embeddings, axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Create a FAISS index\n",
    "    embedding_dim = embeddings.shape[1] \n",
    "    index = faiss.IndexFlatIP(embedding_dim)  \n",
    "    faiss.normalize_L2(embeddings) \n",
    "    index.add(embeddings)  \n",
    "    \n",
    "    # Perform a search\n",
    "    query_embedding = emb_model.encode([tokenize(query)], convert_to_numpy=True)\n",
    "    query_embedding /= np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "    faiss.normalize_L2(query_embedding)    \n",
    "    \n",
    "    distances, indices = index.search(query_embedding, topk)    \n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        result = deepcopy(corpus[idx]) \n",
    "        result[\"score\"] = float(distances[0][i]) \n",
    "        results.append(result)\n",
    "\n",
    "    return results    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some heuristic functions to improve the retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T15:37:15.915311Z",
     "iopub.status.busy": "2024-12-24T15:37:15.915011Z",
     "iopub.status.idle": "2024-12-24T15:37:15.958141Z",
     "shell.execute_reply": "2024-12-24T15:37:15.957413Z",
     "shell.execute_reply.started": "2024-12-24T15:37:15.915277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_consecutive_subarray(numbers):\n",
    "    subarrays = []\n",
    "    current_subarray = []\n",
    "    for num in numbers:\n",
    "        if not current_subarray or num == current_subarray[-1] + 1:\n",
    "            current_subarray.append(num)\n",
    "        else:\n",
    "            subarrays.append(current_subarray)\n",
    "            current_subarray = [num]\n",
    "\n",
    "    subarrays.append(current_subarray)  # Append the last subarray\n",
    "    return subarrays\n",
    "\n",
    "def merge_contexts(passages):\n",
    "    passages_sorted_by_id = sorted(passages, key=lambda x: x[\"id\"])\n",
    "\n",
    "    psg_ids = [x[\"id\"] for x in passages_sorted_by_id]\n",
    "    consecutive_ids = extract_consecutive_subarray(psg_ids)\n",
    "\n",
    "    merged_contexts = []\n",
    "    b = 0\n",
    "    for ids in consecutive_ids:\n",
    "        psgs = passages_sorted_by_id[b:b+len(ids)]\n",
    "        \n",
    "        # Group passages by title within the consecutive IDs\n",
    "        title_groups = {}\n",
    "        for psg in psgs:\n",
    "            title = psg[\"title\"]\n",
    "            if title not in title_groups:\n",
    "                title_groups[title] = []\n",
    "            title_groups[title].append(psg)\n",
    "        \n",
    "        # Merge passages in each title group\n",
    "        for title, group in title_groups.items():\n",
    "            if len(group) == 1:\n",
    "                # If only one passage in the group, add it as is\n",
    "                psg = group[0]\n",
    "                merged_contexts.append(dict(\n",
    "                    title=psg['title'],\n",
    "                    passage=psg['passage'],\n",
    "                    score=psg[\"score\"],\n",
    "                    merged_from_ids=[psg[\"id\"]]\n",
    "                ))\n",
    "            else:\n",
    "                # Merge passages with the same \n",
    "                psg_texts = [x[\"passage\"] for x in group]\n",
    "                merged = \" \".join(psg_texts)\n",
    "                merged_contexts.append(dict(\n",
    "                    title=group[0]['title'],\n",
    "                    passage=merged,\n",
    "                    score=max([x[\"score\"] for x in group]),\n",
    "                    merged_from_ids=[x[\"id\"] for x in group]\n",
    "                ))\n",
    "\n",
    "        b = b + len(ids)\n",
    "\n",
    "    return merged_contexts\n",
    "\n",
    "def discard_contexts(passages):\n",
    "    sorted_passages = sorted(passages, key=lambda x: x[\"score\"])\n",
    "    if len(sorted_passages) == 1:\n",
    "        return sorted_passages\n",
    "    else:\n",
    "        shortened = deepcopy(sorted_passages)\n",
    "        for i in range(len(sorted_passages) - 1):\n",
    "            current, next = sorted_passages[i], sorted_passages[i+1]\n",
    "            if next[\"score\"] - current[\"score\"] >= 0.05:\n",
    "                shortened = sorted_passages[i+1:]\n",
    "        return shortened\n",
    "\n",
    "def expand_context(passage, corpus=corpus, word_window=60, n_sent=3):\n",
    "    # psg_id = passage[\"id\"]\n",
    "    merged_from_ids = passage[\"merged_from_ids\"]\n",
    "    title = passage[\"title\"]\n",
    "    prev_id = merged_from_ids[0] - 1\n",
    "    next_id = merged_from_ids[-1] + 1\n",
    "\n",
    "    texts = []\n",
    "    if prev_id in range(0, len(corpus)):\n",
    "        prev_psg = corpus[prev_id]\n",
    "        if prev_psg[\"title\"] == title:\n",
    "            prev_text = prev_psg[\"passage\"]\n",
    "            # prev_text = \" \".join(prev_text.split()[-word_window:])\n",
    "            prev_text = \" \".join(sent_tokenize(prev_text)[-n_sent:])\n",
    "            texts.append(prev_text)\n",
    "\n",
    "    texts.append(passage[\"passage\"])\n",
    "\n",
    "    if next_id in range(0, len(corpus)):\n",
    "        next_psg = corpus[next_id]\n",
    "        if next_psg[\"title\"] == title:\n",
    "            next_text = next_psg[\"passage\"]\n",
    "            # next_text = \" \".join(next_text.split()[:word_window])\n",
    "            next_text = \" \".join(sent_tokenize(next_text)[:n_sent])\n",
    "            texts.append(next_text)\n",
    "\n",
    "    expanded_text = \" \".join(texts)\n",
    "    new_passage = deepcopy(passage)\n",
    "    new_passage[\"passage\"] = expanded_text\n",
    "    return new_passage\n",
    "\n",
    "def expand_contexts(passages, corpus=corpus, word_window=60, n_sent=3):\n",
    "    new_passages = [expand_context(passage, corpus=corpus) for passage in passages]\n",
    "    return new_passages\n",
    "\n",
    "def collapse(passages):\n",
    "    new_passages = deepcopy(passages)\n",
    "    titles = {}\n",
    "    for passage in new_passages:\n",
    "        title = passage[\"title\"]\n",
    "        if not titles.get(title):\n",
    "            titles[title] = [passage]\n",
    "        else:\n",
    "            titles[title].append(passage)\n",
    "    best_passages = []\n",
    "    for k, v in titles.items():\n",
    "        best_passage = max(v, key= lambda x: x[\"score\"])\n",
    "        best_passages.append(best_passage)\n",
    "    sorted_best_passages = sorted(best_passages, key=lambda x: x[\"score\"], reverse=True)\n",
    "    return sorted_best_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T15:37:15.959224Z",
     "iopub.status.busy": "2024-12-24T15:37:15.958957Z",
     "iopub.status.idle": "2024-12-24T15:37:15.976726Z",
     "shell.execute_reply": "2024-12-24T15:37:15.975877Z",
     "shell.execute_reply.started": "2024-12-24T15:37:15.959196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def smooth_contexts(passages, corpus=corpus):\n",
    "    \"\"\"Make the context fed to the LLM better.\n",
    "    Args:\n",
    "        passages (list): Chunks retrieved from BM25 + semantic retrieval.\n",
    "\n",
    "    Returns:\n",
    "        list: List of whole paragraphs, usually will be more relevant to the initital question.\n",
    "    \"\"\"\n",
    "    # 1. If consecutive chunks are retrieved, merge them into one big chunk to ensure the continuity.\n",
    "    merged_contexts = merge_contexts(passages)\n",
    "    # 2. A heuristic to discard irrelevevant contexts.\n",
    "    # It seems to be better to only keep what are elevant so that the model can focus.\n",
    "    # Also this reduce #tokens LLM has to read.\n",
    "    shortlisted_contexts = discard_contexts(merged_contexts)\n",
    "    # 3. Another heuristic. this step is to take advantage of long context understanding of the LLM.\n",
    "    # In many cases, the retrieved passages are just consecutive words, not a comprehensive paragraph.\n",
    "    # This is to expand the passage to the whole paragraph that surrounds it.\n",
    "    # My intuition about this is that whole paragraph will add necessary and relevant information.\n",
    "    expanded_contexts = expand_contexts(shortlisted_contexts, corpus=corpus)\n",
    "    # 4. Now after all the merging and expanding, if what are left for us is more than one paragraphs\n",
    "    # from the same URL, then we will only take paragraph with highest retrieval score.\n",
    "    collapsed_contexts = collapse(expanded_contexts)\n",
    "    return collapsed_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T15:37:15.977753Z",
     "iopub.status.busy": "2024-12-24T15:37:15.977515Z",
     "iopub.status.idle": "2024-12-24T15:37:16.834854Z",
     "shell.execute_reply": "2024-12-24T15:37:16.833913Z",
     "shell.execute_reply.started": "2024-12-24T15:37:15.977721Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 202.28it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c5b27b0e4f4f6da830f4fe03c62f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e21d51e87e4e32a71e9eb08dc100c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_passages = retrieve('pickleball là gì')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T15:37:16.836174Z",
     "iopub.status.busy": "2024-12-24T15:37:16.835846Z",
     "iopub.status.idle": "2024-12-24T15:37:16.841771Z",
     "shell.execute_reply": "2024-12-24T15:37:16.840977Z",
     "shell.execute_reply.started": "2024-12-24T15:37:16.836141Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Pickleball là gì? Nguồn gốc, cách chơi và những điều cần biết',\n",
       "  'passage': 'Pickleball là gì? Nguồn gốc, cách chơi và những điều cần biết Facebook Instagram Mua vé sự kiện Virtual Race Sự Kiện tại Việt Nam Dịch vụ Bán Vé Sự Kiện Tìm Hình Sự Kiện Tổ Chức Sự Kiện Kiếm Tiền cùng iRace.vn Sản Xuất Vật Phẩm Huy chương Quần áo, Phụ kiện Thông Tin Về iRace Báo Chí Viết Hỏi Đáp Hướng Dẫn Sử Dụng Kiến Thức Chạy Bộ Shop Thể Thao Dinh Dưỡng Phụ kiện Đăng nhập Tiếng Việt TÌM KIẾM Đăng nhập Đăng nhập tài khoản Tài khoản mật khẩu của bạn Forgot your password? Get help Điều khoản sử dụng Khôi phục mật khẩu Khởi tạo mật khẩu email của bạn Mật khẩu đã được gửi vào email của bạn. iRace.vn Nền tảng tổ chức chạy bộ trực tuyến 1 Việt Nam Mua vé sự kiện Virtual Race Sự Kiện tại Việt Nam Dịch vụ Bán Vé Sự Kiện Tìm Hình Sự Kiện Tổ Chức Sự Kiện Kiếm Tiền cùng iRace.vn Sản Xuất Vật Phẩm Huy chương Quần áo, Phụ kiện Thông Tin Về iRace Báo Chí Viết Hỏi Đáp Hướng Dẫn Sử Dụng Kiến Thức Chạy Bộ Shop Thể Thao Dinh Dưỡng Phụ kiện Đăng nhập Tiếng Việt Trang chủTin tức thể thaoPickleball là gì? Nguồn gốc, cách chơi và những điều cần biết Tin tức thể thao Pickleball là gì? Nguồn gốc, cách chơi và những điều cần biết FacebookLinkedinEmailPrintTelegramViber Mục lục bài viết Toggle Pickleball là gì ?Tại sao Pickleball lại trở thành cơn sốt tại Việt Nam?Luật chơi cơ bản cho người mới bắt đầu tham gia PickleballQuy định về vợt và sân chơi PickleballCác câu hỏi thường gặp khi chơi PickleballLoại vợt và bóng để chơi Pickleball là gì ?Ở Tp.HCM và Hà Nội có những sân chơi Pickleball nào không?Pickleball khác với tennis như thế nào?Sự khác biệt giữa pickleball ngoài trời và pickleball trong nhà là gì?Làm thế nào để bạn giành chiến thắng trong trò pickleball?Làm thế nào để tôi có thể cải thiện kỹ năng chơi pickleball của mình? Pickleball là một từ khóa nổi lên rất mạnh gần đây và gây nên cơn sốt mạnh mẽ ở Việt Nam, vậy Pickleball là gì, nó có nguồn gốc từ đâu, luật chơi có dễ không và đối tượng của bộ môn này là ai, hôm nay hãy cùng tìm iRace tìm hiểu nó nhé. Pickleball là gì ? Pickleball là một môn thể thao kết hợp giữa tennis, bóng bàn và cầu lông, được chơi trên sân có kích thước tương tự sân cầu lông. Người chơi sử dụng vợt nhỏ hơn so với tennis và bóng có lỗ, tương tự như bóng wiffle. Môn thể thao này có thể chơi đơn hoặc đôi, và quy tắc chơi khá đơn giản, phù hợp với mọi lứa tuổi. Với sự kết hợp của các yếu tố từ các môn thể thao khác nhau, Pickleball mang đến một trải nghiệm thú vị và đầy thử thách cho người chơi. Pickleball ra đời năm 1965, do Joel Pritchard cố Hạ nghị sĩ Mỹ, cùng hai người bạn là Bill Bell và Barney McCallum sáng tạo, ban đầu nhằm mục đích giải trí cho gia đình và bạn bè trên đảo Bainbridge, bang Washington, Mỹ. Từ trái qua Barney McCallum, Joel Pritchard, Bill Bell là 3 người bạn đã khởi đầu ý tưởng về pickleball. Ảnh Pickleball Popularity Trò chơi nhanh chóng phát triển và lan rộng khắp nước Mỹ, dẫn đến việc thành lập Hiệp hội Pickleball Mỹ vào năm 1976. Từ đó, Pickleball lan tỏa ra hơn 50 quốc gia, bao gồm Việt Nam. Tên gọi Pickleball xuất phát từ con chó của Pritchard, tên Pickle, đã chạy lấy quả bóng trong lúc họ chơi. Ban đầu môn thể thao có tên là Pickles ball (bóng của Pickle). Dần dần, người ta chỉ gọi nó là Pickleball và cái tên này đã trở thành tên gọi chính thức cho môn thể thao này. Tại sao Pickleball lại trở thành cơn sốt tại Việt Nam? Pickleball không chỉ thu hút bởi tính đơn giản và dễ tiếp cận, mà còn bởi vì nó là một môn thể thao lý tưởng cho mọi lứa tuổi và mọi trình độ. Với lối chơi nhanh, ít đòi hỏi về thể lực so với các môn thể thao khác, Pickleball phù hợp cho cả những người lớn tuổi và người mới bắt đầu. Hơn nữa, sự lan rộng của môn thể thao này trong các cộng đồng và các trường học đã tạo ra một cộng đồng Pickleball ngày càng lớn mạnh. Những giải đấu và sự kiện Pickleball cũng ngày càng được tổ chức nhiều, giúp lan tỏa môn thể thao này trên toàn thế giới. Chơi Pickleball không chỉ mang lại niềm vui mà còn đem lại nhiều lợi ích về sức khỏe và tinh thần. Đầu tiên, Pickleball là một môn thể thao tuyệt vời giúp tăng cường thể lực và sức bền, đồng thời cải thiện khả năng phản xạ và sự linh hoạt. Thứ hai, việc tham gia chơi Pickleball thường xuyên giúp giảm căng thẳng, tăng cường sự tập trung và cải thiện tâm trạng. Ngoài ra, đây còn là cơ hội tuyệt vời để gặp gỡ và giao lưu với những người cùng sở thích, giúp bạn xây dựng mối quan hệ xã hội và mở rộng mạng lưới bạn bè. Với những lợi ích đa dạng và phong phú này, Pickleball chắc chắn là một môn thể thao đáng để thử và tham gia. Luật chơi cơ bản cho người mới bắt đầu tham gia Pickleball Quy định về vợt và sân chơi Pickleball Đầu tiên là vợt Pickleball, nó có tay cầm và vợt hình chữ nhật to hơn vợt bóng bàn một chút và chiều dài không được vượt quá 43.18cm và chiều rộng tối đa la 60.96cm. Bóng Pickleball là loại bóng nhựa cứng, đường kính khoảng 74-76mm và nặng khoảng 22-26g, khả năng nảy của loại bóng này không bằng bóng tennis. Tiếp theo là phần sân Pickleball. Theo quy định nó có kích thước tiêu chuẩn là 13.4m x 6.1m. Nhìn qua thì nó tương đương với sân cầu lông. Sân Pickleball cũng yêu cầu phải có một vùng an toàn quanh sân ít nhất là 1.5m, tổng diện tích sử dụng tiêu chuẩn để đảm bảo an toàn sẽ là 18.3m x 9.1m. Sân sẽ được chia làm 3 phần bao gồm phần ở giữa được chia đôi bởi lưới ngăn. Mỗi bên lưới sẽ có độ rộng 2.13m được gọi là khu vực cấm vô lê (hay còn gọi là kitchen). Nếu người chơi đứng trong khu vực này thì không được đánh bóng cho đến khi bóng nảy. Điểm sẽ được tính khi bóng ra ngoài sân và người giao bóng được ghi điểm. Số người có thể chơi Về số lượng người chơi thì mõi bên có thể đánh đơn hoặc đánh đôi, khi chơi đôi thì 2 người sẽ đứng cùng 1 bên khi giao bóng. Khi giao bóng Khi bắt đầu, người giao bóng phải đánh bóng qua lưới bằng vợt, điểm sẽ được tính cho đối thủ nếu giao bóng ra ngoài sân hoặc chạm bóng trong khu vực kitchen. Trong khi giao bóng, người chơi phải giữ cả 2 chân ngoài khu vực sân Pickleball và bóng phải đánh từ mức ngang eo lên cao và giao chéo sân, bóng phải được nảy 1 lần trước khi được đánh trả, khi đánh bóng, có thể được đánh không chạm đất nhưng giữa sân có vùng không cho phép đánh trả bóng trên không. Một hiệp đánh Pickleball kết thúc Khi một đội chơi đơn đạt được 11 điểm trước và chênh lệch với đối thủ tối thiểu 2 điểm thì hiệp đấu kết thúc. Trong trường hợp cả 2 đội đang cùng điểm 10 thì trận đấu sẽ tiếp tục cho đến khi có 1 đội có điểm cách biệt là 2 so với đội còn lại. Đối với người chơi Pickleball đôi thì điểm số là 15 hoặc 21. Cách tính điểm khi chơi Pickleball nhìn chung khá đơn giản. Đội của bạn sẽ được điểm khi Đội kia để bóng chạm hoặc rơi vào khu vực không chơi Người chơi của đội đối thủ vi phạm quy tắc như đánh bóng ra ngoài san hoặc đánh trúng người đội bạn Đội của đối thủ sẽ được điểm khi Đội của bạn phạm lỗi Không giao bóng qua vạch khu vực kitchen Đánh bóng ra ngoài sân hoặc iRace sẽ có một bài viết chi tiết về cách chơi Pickleball để bạn có thể có cái nhìn rõ hơn trong bài tiếp theo. Các câu hỏi thường gặp khi chơi Pickleball Loại vợt và bóng để chơi Pickleball là gì ? Loại vợt để chơi Pickleball nó gần giống vợt bóng bàn như to hơn một chút, nó thường được sản xuất từ gỗ, nhựa hoặc cao cấp hơn là từ sợi các-bon, sợi thủy tinh và có nhiều lỗ nhỏ để giúp tăng tốc và giảm trọng lượng vợt. Vợt Pickleball làm từ nhiều chất liệu khác nhau nên trọng lượng cũng khá khác nhau. Bạn nên lựa chọn vợt phù hợp để có thể chơi môn này được thoải mái nhất. Về bóng Pickleball thì nó thường làm từ nhựa cứng, có nhiều lỗ nhỏ (như pho mát) trên bề mặt bóng, do nhẹ nên nó có độ nảy thấp hơn đa số loại bóng khác. Bóng dùng trong nhà thường sẽ mềm hơn, ít lỗ hơn bóng chơi ngoài trời để chống lại yếu tố môi trường như gió. Ở Tp.HCM và Hà Nội có những sân chơi Pickleball nào không? Nếu bạn đang có ý định trải nghiệm môn Pickleball siêu hot này thì hãy đến ngay một số địa chỉ sau nha Tại Tp.HCM Sân Cung Văn Hóa Lao Động (Sân Pickleball 102 Club) 55B Nguyễn Thị Minh Khai, Phường Bến Thành, Quận 1 Sân 002 Pickleball 28 Thảo Điền, Phường Thảo Điền, Quận 2 Sân Pickleball Vườn Lan 21 đường số 34, Phường 10, Quận 6 Sân Câu Lạc Bộ Lan Anh 291 Cách Mạng Tháng 8, Phường 12, Quận 10 Sân Rudal Pickleball 28 đường 12, Phường An Khánh, TP. Thủ Đức Sân Pickleball Phú Nhuận 3B Lê Quý Đôn, Phường 11, Quận Phú Nhuận Sân Pickleball Tân Phú Số 130, đường Chế Lan Viên, Tây Thạnh, quận Tân Phú Tại Hà Nội Sân Pickleball Hà Nội Bách Khoa số 42, đường Tạ Quang Bửu, Phường Bách Khoa, Quận Hai Bà Trưng, Hà Nội Sân Etennis Pickleball Viet Hoàng Mai 6 Kim Đồng, phường Giáp Bát, quận Hoàng Mai, Hà Nội Sân Pickleball Cầu Giấy35 Trần Quý Kiên, Dịch Vọng, Cầu Giấy, Hà Nội Sân Pickleball Pro Pickleball Vn Số 128, Ngõ 1, Phố Phú Viên, Phường Bồ Đề, Quận Long Biên, Hà Nội Sân PickleBall Hà Nội Số 1 Phú Viên Số 1, phố Phú Viên, Phường Bồ Đề, Quận Long Biên, Hà Nội Pickleball khác với tennis như thế nào? Pickleball được chơi trên sân nhỏ hơn tennis và có lưới thấp hơn. Quả bóng được sử dụng trong pickleball được làm bằng nhựa nhẹ và có lỗ đục xuyên qua, khiến nó chậm hơn và dễ kiểm soát hơn so với bóng tennis. Pickleball cũng được chơi bằng vợt chắc chắn thay vì vợt có dây. Trong pickleball, bóng được giao bóng dưới tay, trong khi tennis yêu cầu giao bóng trên tay. Sự khác biệt giữa pickleball ngoài trời và pickleball trong nhà là gì? Bóng pickleball ngoài trời có 40 lỗ và bóng pickleball trong nhà có 26 lỗ. Cả hai loại bóng đều trông giống nhau và có thể có bất kỳ màu nào để dễ nhìn hoặc điều kiện sân. Làm thế nào để bạn giành chiến thắng trong trò pickleball? Mục tiêu của pickleball là ghi điểm bằng cách đánh bóng qua lưới và trong ranh giới của sân, và ngăn cản đối thủ của bạn làm điều tương tự. Người chơi hoặc đội đầu tiên đạt được 11 điểm và có ít nhất hai điểm dẫn trước sẽ giành chiến thắng trong trò chơi. Làm thế nào để tôi có thể cải thiện kỹ năng chơi pickleball của mình? Luyện tập thường xuyên để cải thiện kỹ năng chơi pickleball của bạn. Học từ một huấn luyện viên được chứng nhận và xem video hướng dẫn hoặc tham gia các buổi hội thảo. Bạn cũng có thể thử chơi với những đối thủ giỏi hơn mình để thử thách bản thân và cải thiện kỹ năng của mình. Ngoài ra, rèn luyện thể lực và di chuyển chân cũng có thể giúp cải thiện trò chơi pickleball của bạn. Lời kết Nhìn chung pickleball không phải là một bộ môn thể thao mới trên thế giới, nó đã được biết đến từ rất lâu nhưng gần đây mới bắt đầu phát triển mạnh ở Việt Nam, đây là một bộ môn dễ chơi, dành cho mọi đối tượng và lứa tuổi. TagsPickleball FacebookLinkedinEmailPrintTelegramViber Bài trướcHướng dẫn gửi kết quả chạy lên Google DriveBài tiếp theo7 luật chơi môn pickleball cơ bản mà bạn cần nắm rõ Bài viết liên quan 7 luật chơi môn pickleball cơ bản mà bạn cần nắm rõ Bạn thấy người người nhà nhà đang chơi pickleball và bạn cũng muốn thử bộ môn siêu HOT này... Có thể bạn thích 7 luật chơi môn pickleball cơ bản mà bạn cần nắm rõ Bạn thấy người người nhà nhà đang chơi pickleball và bạn cũng muốn thử bộ môn siêu HOT này nhưng lại không biết luật... CÔNG TY CỔ PHẦN IRACE - Hotline 0902.31.68.31 - Email supportirace.vn B-00.02 Sarica, KĐT Sala, Đường D9, P. An Lợi Đông, TP. Thủ Đức, TP. HCM Mã số doanh nghiệp 0315105285 do Sở Kế hoạch và Đầu tư TP.HCM cấp lần 2 ngày 12/04/19 Chính sách bảo mật Điều khoản thanh toán Điều khoản sử dụng Tuyển dụng Liên Hệ 2024 iRace.vn - Made by Vietnam . . X',\n",
       "  'score': 0.8357918858528137,\n",
       "  'merged_from_ids': [12, 13, 14, 15, 16, 17]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth_contexts(top_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T15:37:16.842808Z",
     "iopub.status.busy": "2024-12-24T15:37:16.842515Z",
     "iopub.status.idle": "2024-12-24T15:40:29.589039Z",
     "shell.execute_reply": "2024-12-24T15:40:29.588360Z",
     "shell.execute_reply.started": "2024-12-24T15:37:16.842777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e6f45fe58d4be586f5523bf86c5bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bc65d7f0724ddcbb55dbcc2accfdce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_mpt.py:   0%|          | 0.00/16.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d20712264d04655bf1c9c71467a1465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "attention.py:   0%|          | 0.00/24.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fa1afa34314a719ee697983fcda44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "flash_attn_triton.py:   0%|          | 0.00/28.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B-Chat:\n",
      "- flash_attn_triton.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6ea211d1d24b4a8c2975a76fa24837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fc.py:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B-Chat:\n",
      "- fc.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477051661bf94fe2bfccc5a20a1555c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "norm.py:   0%|          | 0.00/3.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B-Chat:\n",
      "- norm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B-Chat:\n",
      "- attention.py\n",
      "- flash_attn_triton.py\n",
      "- fc.py\n",
      "- norm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9bb6c6eb664fd3881f3bace7edf60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ffn.py:   0%|          | 0.00/5.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B-Chat:\n",
      "- ffn.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c122d51e5c44fbb9cf641f40e2cedfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "blocks.py:   0%|          | 0.00/4.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B-Chat:\n",
      "- blocks.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58d6cc17c1d479197a60304e3edf5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "warnings.py:   0%|          | 0.00/894 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B-Chat:\n",
      "- warnings.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B-Chat:\n",
      "- configuration_mpt.py\n",
      "- attention.py\n",
      "- ffn.py\n",
      "- blocks.py\n",
      "- warnings.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B-Chat/3b11fad96d853b78f2fecc76f13aedb381b02d5a/configuration_mpt.py:114: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\n",
      "  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\n",
      "/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B-Chat/3b11fad96d853b78f2fecc76f13aedb381b02d5a/configuration_mpt.py:141: UserWarning: If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".\n",
      "  warnings.warn(UserWarning('If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".'))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e748701a2ca43c99d976cd40f974872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_mpt.py:   0%|          | 0.00/32.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073164ee72104de69757ad26af317f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "custom_embedding.py:   0%|          | 0.00/292 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B-Chat:\n",
      "- custom_embedding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe484d38fee0442e83f64ea9bb2454ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapt_tokenizer.py:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B-Chat:\n",
      "- adapt_tokenizer.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb38a6a048c74d9faec84bc2c4734302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "meta_init_context.py:   0%|          | 0.00/3.96k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B-Chat:\n",
      "- meta_init_context.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8383e53033ee44a1889663339d546eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "param_init_fns.py:   0%|          | 0.00/11.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B-Chat:\n",
      "- param_init_fns.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c5dc2327044dd58ef3aeab86867f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hf_prefixlm_converter.py:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B-Chat:\n",
      "- hf_prefixlm_converter.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B-Chat:\n",
      "- modeling_mpt.py\n",
      "- custom_embedding.py\n",
      "- adapt_tokenizer.py\n",
      "- meta_init_context.py\n",
      "- param_init_fns.py\n",
      "- hf_prefixlm_converter.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9edd0ce8c3d1400ab89042e1f1f449df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/7.38G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea92e1f3e7424a48a61564e088e354e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/91.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5933c4d45142f585730a6c50ab51ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/850 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711bab7dc8324647ba3e52ddae470572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/844k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5720b874b642139ad82673da20296e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "LLM_model_path = \"vinai/PhoGPT-4B-Chat\"  \n",
    "\n",
    "config = AutoConfig.from_pretrained(LLM_model_path, trust_remote_code=True)\n",
    "\n",
    "LLM_model = AutoModelForCausalLM.from_pretrained(LLM_model_path, config=config, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "LLM_model.to(\"cuda\")\n",
    "LLM_model.eval()  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_model_path, trust_remote_code=True)  \n",
    "\n",
    "PROMPT_TEMPLATE = \"### Câu hỏi: {instruction}\\n### Trả lời:\"  \n",
    "\n",
    "# Some instruction examples\n",
    "# instruction = \"Viết bài văn nghị luận xã hội về {topic}\"\n",
    "# instruction = \"Viết bản mô tả công việc cho vị trí {job_title}\"\n",
    "# instruction = \"Sửa lỗi chính tả:\\n{sentence_or_paragraph}\"\n",
    "# instruction = \"Dựa vào văn bản sau đây:\\n{text}\\nHãy trả lời câu hỏi: {question}\"\n",
    "# instruction = \"Tóm tắt văn bản:\\n{text}\"\n",
    "# instruction = \"Sửa lỗi chính tả:\\nTriệt phá băng nhóm kướp ô tô, sử dụng \\\"vũ khí nóng\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T15:40:29.590105Z",
     "iopub.status.busy": "2024-12-24T15:40:29.589849Z",
     "iopub.status.idle": "2024-12-24T15:40:29.596482Z",
     "shell.execute_reply": "2024-12-24T15:40:29.595673Z",
     "shell.execute_reply.started": "2024-12-24T15:40:29.590081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def chat_RAG(search_topk=3, rag_topk=10):\n",
    "    search_query = input(\"Please input your search query. Enter q to quit >\")\n",
    "    if search_query == \"q\":\n",
    "        pass\n",
    "    docs = search_google(search_query, search_topk)\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=2000, \n",
    "    chunk_overlap=0,\n",
    "    separator=\" \",\n",
    "    length_function=len\n",
    ")\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "    corpus = splits_to_dicts(texts)\n",
    "    while True:\n",
    "        chat_query = input(\"Please input your query. Enter q to quit >\")\n",
    "        if input == \"q\":\n",
    "            return\n",
    "        top_passages = retrieve(chat_query, corpus, rag_topk)\n",
    "        contexts = smooth_contexts(top_passages, corpus)\n",
    "        instruction = \"Dựa vào văn bản sau đây:\\n{text}\\nHãy trả lời câu hỏi: {question}\".format_map({\"text\": contexts, \"question\": chat_query})\n",
    "        input_prompt = PROMPT_TEMPLATE.format_map({\"instruction\": instruction}) \n",
    "        input_ids = tokenizer(input_prompt, return_tensors=\"pt\")\n",
    "        outputs = LLM_model.generate(  \n",
    "        inputs=input_ids[\"input_ids\"].to(\"cuda\"),  \n",
    "        attention_mask=input_ids[\"attention_mask\"].to(\"cuda\"),  \n",
    "        do_sample=True,  \n",
    "        temperature=1.0,  \n",
    "        top_k=50,  \n",
    "        top_p=0.9,  \n",
    "        max_new_tokens=1024,  \n",
    "        eos_token_id=tokenizer.eos_token_id,  \n",
    "        pad_token_id=tokenizer.pad_token_id)  \n",
    "        response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  \n",
    "        response = response.split(\"### Trả lời:\")[1]\n",
    "        print(response)\n",
    "        print(\"---\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T15:40:29.597334Z",
     "iopub.status.busy": "2024-12-24T15:40:29.597145Z",
     "iopub.status.idle": "2024-12-24T16:03:36.044862Z",
     "shell.execute_reply": "2024-12-24T16:03:36.043501Z",
     "shell.execute_reply.started": "2024-12-24T15:40:29.597317Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input your search query. Enter q to quit > đại học Bách Khoa Hà Nội\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently searching the website: https://hust.edu.vn/\n",
      "Currently searching the website: https://vi.wikipedia.org/wiki/%C4%90%E1%BA%A1i_h%E1%BB%8Dc_B%C3%A1ch_khoa_H%C3%A0_N%E1%BB%99i\n",
      "Currently searching the website: https://xaydungchinhsach.chinhphu.vn/diem-chuan-dai-hoc-bach-khoa-ha-noi-11924081717400873.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input your query. Enter q to quit > đại học Bách Khoa Hà Nội được thành lập năm nào, do ai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 185.95it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364a13f76b3d451a9416a1d638c8c2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531d7efe441c4b65b2831678591730f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B-Chat/3b11fad96d853b78f2fecc76f13aedb381b02d5a/attention.py:87: UserWarning: Propagating key_padding_mask to the attention module and applying it within the attention module can cause unnecessary computation/memory usage. Consider integrating into attn_bias once and passing that to each attention module instead.\n",
      "  warnings.warn('Propagating key_padding_mask to the attention module ' + 'and applying it within the attention module can cause ' + 'unnecessary computation/memory usage. Consider integrating ' + 'into attn_bias once and passing that to each attention ' + 'module instead.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đại học Bách khoa Hà Nội được thành lập theo Nghị định số 147/NĐ ngày 6-3-1956 do Bộ trưởng Bộ Giáo dục Nguyễn Văn Huyên ký.\n",
      "Bách khoa Hà Nội được thành lập theo Nghị định số 147/NĐ ngày 6-3-1956 do Bộ trưởng Bộ Giáo dục Nguyễn Văn Huyên ký.\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input your query. Enter q to quit > giới thiệu qua về đại học Bách Khoa Hà Nội\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 162.53it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56118b2b33e9431993505f9a8fb2e2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eebcb63150148f6a2246a275a73b4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giới thiệu về Đại học Bách khoa Hà Nội (BKHN) là một trường đại học kỹ thuật và công nghệ hàng đầu tại Việt Nam, được biết đến với các chuyên ngành kỹ thuật và công nghệ. Đại học Bách khoa Hà Nội được thành lập theo Nghị định số 196-CP ngày 26 tháng 10 năm 1956 của Bộ Quốc phòng. Trường là thành viên của Hiệp hội các đại học kỹ thuật hàng đầu châu Á - Thái Bình Dương (AOTULE) từ năm 2015. BKHN sở hữu đội ngũ cán bộ, giảng viên và công nhân viên đông đảo, bao gồm các giảng viên và cựu giảng viên giàu kinh nghiệm và tâm huyết, cùng với một hệ thống cơ sở vật chất hiện đại và một cơ sở dữ liệu phong phú về các tài liệu nghiên cứu và các ấn phẩm.\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input your query. Enter q to quit > q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 172.76it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdf94e8755c46b790cc97e1bc7de520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67907a599d8647599655b07426780afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-396082611a3e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchat_RAG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-b720d32455ef>\u001b[0m in \u001b[0;36mchat_RAG\u001b[0;34m(search_topk, rag_topk)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0minput_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPROMPT_TEMPLATE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"instruction\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minstruction\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         outputs = LLM_model.generate(  \n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m             \u001b[0;31m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2024\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2025\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2026\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   3018\u001b[0m                 \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3019\u001b[0m                 \u001b[0;31m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3020\u001b[0;31m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3021\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3022\u001b[0m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chat_RAG()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
